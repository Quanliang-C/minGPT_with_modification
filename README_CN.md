[English Version](README.md)

# minGPT_with_modification

CS224N 项目：基于预训练 Transformer 和 RoPE 的知识问答

## 目录

- 项目概述
- 核心技术与架构
- 基础模型：minGPT
- 预训练任务：Span Corruption
- 位置编码探索：从可学习到 RoPE
- 实验与结果分析
- 实验设置
- 实验一：无预训练直接微调
- 实验二：标准预训练与微调 (Vanilla Transformer)
- 实验三：基于 RoPE 的预训练与微调
- 结果汇总
- 结论

## 项目概述

本项目源于斯坦福大学的 CS224N: NLP with Deep Learning 课程作业，旨在深入探究预训练（Pre-training）对 Transformer 模型在知识密集型任务中表现的影响。项目基于 Andrej Karpathy 的 minGPT 库进行构建和扩展。

核心任务是一个简单的“问题回答”：给定一个名人的名字，模型需要预测其出生地。例如：

问题: "Where was [person] born?"
回答: "[place]"

这个任务的挑战在于，模型的答案无法从输入中直接推断，必须依赖其在训练过程中学到的“世界知识”。本项目通过以下几个关键探索，验证了预训练的强大能力，并实现了一种更先进的位置编码方案：

- 实现了基于 T5 论文的 Span Corruption 预训练任务，使模型能够从大规模无标签文本（维基百科）中学习事实知识。
- 实现了旋转位置编码 (Rotary Positional Embedding, RoPE)，作为对传统可学习绝对位置编码的改进，并将其成功应用于模型中。
- 通过三组对比实验，清晰地展示了从零训练、标准预训练到使用 RoPE 预训练的性能演进。

## 核心技术与架构

### 基础模型：minGPT

本项目采用了一个 GPT 架构的 Transformer 模型作为基础，该模型基于 minGPT 实现。minGPT 是一个简洁、清晰的 Transformer 实现，便于进行学术研究和二次开发。我们在此基础上进行了修改，以集成新的预训练任务和位置编码机制。

### 预训练任务：Span Corruption

为了让模型学习世界知识，我们引入了预训练步骤。受 T5 论文 (Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer) 的启发，我们实现了 Span Corruption 作为预训练任务。

其核心思想是：

- 从输入文本中随机选择一个连续的文本片段 (span)。
- 用一个特殊的 [MASK] 标记替换这个片段。
- 模型的任务是根据上下文，准确地预测出被遮盖的原始文本片段。

通过在庞大的维基百科语料库上执行此任务，模型被迫学习并内化语言结构、语法以及最重要的——事实知识（例如，特定人物和其出生地的关联）。这是模型后续能够在问答任务中表现出色的关键。

### 位置编码探索：从可学习到 RoPE

Transformer 模型本身不具备感知序列顺序的能力，因此需要引入位置编码。本项目对比了两种不同的方案。

1. Vanilla Transformer: 可学习的绝对位置编码

这是 minGPT 默认使用，也是许多早期 Transformer 模型（如 BERT, GPT-2）采用的经典方案。模型为每个绝对位置（第1、2、3...个词）学习一个独立的位置向量，并将其加到词嵌入上。这种方法的缺点是泛化能力有限，当遇到比训练时更长的序列时，模型可能会因为没有见过相应位置的编码而表现不佳。

2. RoPE: 旋转位置编码 (Rotary Positional Embedding)

作为本项目的核心创新点之一，我们亲手实现了 RoPE (Roformer: Enhanced Transformer with Rotary Position Embedding)，并用它取代了原有的可学习位置编码。

RoPE 的关键优势在于它是一种相对位置编码：

- 它不是将位置信息“加”到词嵌入上，而是通过旋转操作将位置信息“乘”入模型的 Query 和 Key 向量中。
- 对任意两个位置的词向量进行点积操作时，其结果只与它们的相对距离有关，而与绝对位置无关。

这使得模型能够更好地理解词与词之间的相对关系，并能更平滑地泛化到未见过的序列长度。

我们的实现遵循了 RoPE 的原始论文，在自注意力模块的每个头中，对 Query 和 Key 向量应用旋转变换。

## 模型配置与关键超参数

| 参数 | 数值 | 说明 |
|------|------|------|
| `n_layer` | **4** | Transformer Block 数量 |
| `n_head`  | **8** | 每层注意力头数 |
| `n_embd`  | **256** | 词向量 / 隐层维度 |
| `block_size` | **128 tokens** | 最大上下文长度 |
| `vocab_size` | **256 字符** | 由 `wiki.txt` 动态构建 |

其他重要训练配置（见 `run.py` & `trainer.py`）：

```python
# 预训练（默认）
max_epochs   = 650
batch_size   = 128
learning_rate= 6e-3

# 微调（加载预训练）
max_epochs   = 15
batch_size   = 256
learning_rate= 6e-4
```

`trainer.TrainerConfig` 还包括权重衰减、梯度裁剪 (`1.0`)、余弦退火 + 预热 (`warmup_tokens = 512*20`)，并自动记录 TensorBoard 日志。

### 自定义数据集

| 数据集 | 用途 | 长度 | 亮点 |
|--------|------|------|------|
| `CharCorruptionDataset` | Span Corruption 预训练 | 128 | T5 风格遮蔽：`prefix ⁇ suffix ⁇ masked_span + PAD` |
| `NameDataset` | 名人-出生地监督微调 | 127 | 在 `y` 中屏蔽问题文本，仅对答案反向传播 |

两个阶段共用同一词汇表，保证嵌入空间一致性。

## 实验与结果分析

### 实验设置

预训练数据: 英文维基百科 (wiki.txt)
微调/评估数据: 名人-出生地配对数据集 (birth_places_train.tsv, birth_dev.tsv)
评估指标: 在开发集（dev set）上的预测准确率 (Accuracy)。

### 实验一：无预训练直接微调

在这个基线实验中，我们直接在一个随机初始化的 Transformer 模型上进行微调，而不经过任何预训练。

目的: 验证模型在没有先验知识的情况下，能否仅从微调数据中学会任务。

结果: 正确率 1.8% (500个样本中答对9个)。

分析: 这个结果接近于随机猜测。这清晰地表明，模型无法凭空“创造”知识。微调数据集本身不足以让模型学习到人名和地名之间的真实关联，它只能进行有限的模式匹配和记忆，导致性能极差。

### 实验二：标准预训练与微调 (Vanilla Transformer)

在这个实验中，我们首先使用 Span Corruption 任务在维基百科上对标准 Transformer（使用可学习位置编码）进行预训练，然后再在名人-出生地数据集上进行微调。

目的: 验证预训练对知识密集型任务的有效性。

预训练参数: batch_size=256, epochs=1200
微调参数: batch_size=512, epochs=50

结果: 正确率 24.0% (500个样本中答对120个)。

分析: 性能实现了质的飞跃。这强有力地证明了预训练的价值。通过在维基百科上进行大规模的无监督学习，模型成功地将大量事实知识编码到了其参数中。在微调阶段，模型学会了如何“提取”和利用这些已存储的知识来回答问题。

### 实验三：基于 RoPE 的预训练与微调

这是我们的核心实验，将标准模型中的可学习位置编码替换为我们实现的 RoPE，并重复预训练和微调的完整流程。

目的: 评估我们实现的 RoPE 相对于传统位置编码的性能。

预训练参数: batch_size=128, epochs=650
微调参数: batch_size=256, epochs=15

结果: 正确率 24.4% (500个样本中答对122个)。

分析: 使用 RoPE 的模型不仅成功运行，并且取得了比 Vanilla Transformer 更高的准确率。这表明，我们实现的 RoPE 是正确且有效的。相对位置编码方案在本任务中展现出了微弱的优势，证明了其作为一种更先进的位置编码技术的潜力。

### 结果汇总

| 实验方案 | 位置编码方式 | 预训练？ | 开发集准确率 |
|------------|--------------|----------|--------------|
| 1. 直接微调 (Finetune Only) | 可学习 (Learnable) | 否 | 1.8% |
| 2. 预训练+微调 (Pretrain + Finetune) | 可学习 (Learnable) | 是 | 24.0% |
| 3. RoPE 预训练+微调 (RoPE Pretrain) | 旋转式 (RoPE) | 是 | 24.4% |

## 结论

本项目成功地复现并验证了预训练在赋予 Transformer 模型世界知识方面的关键作用。通过一系列对比实验，我们得出以下结论：

- 预训练是不可或缺的：对于需要外部知识的 NLP 任务，没有经过预训练的 Transformer 模型几乎无法完成任务。
- Span Corruption 是有效的预训练方法：该方法能有效促使模型学习并记忆文本中的事实信息。
- RoPE 是一种优秀的替代方案：我们成功实现的旋转位置编码（RoPE）在性能上略优于传统的可学习位置编码，验证了相对位置信息对于此类任务的价值和我们工程实现的正确性。

总而言之，这个项目不仅是对 CS224N 课程知识的实践，更是一次对 Transformer 前沿技术（如 RoPE 和 T5 风格的预训练）的成功探索与实现。

<a name="english-version"></a>English Version
[English Version](README.md) 